{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966245cf-f726-40f6-8465-b5bd9b33484a",
   "metadata": {},
   "source": [
    "# ðŸ§  02. Model Architecture\n",
    "**Objective:** Build the Neural Network structure (DNA-BERT).\n",
    "\n",
    "**What this notebook does:**\n",
    "1.  **Tokenization:** Creates a custom tokenizer to convert DNA (\"ATCG\") into numbers.\n",
    "2.  **Dataset Class:** Wraps our `training_manifest.csv` into a PyTorch-ready data loader.\n",
    "3.  **Model Design:** Defines the Transformer architecture (Encoder-only, like BERT).\n",
    "4.  **Verification:** Runs a \"Forward Pass\" on a single batch of data to prove the shapes align."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff00bcf-61d1-4723-9687-4c1c65bcbd14",
   "metadata": {},
   "source": [
    "### 1. Environment & Device Setup\n",
    "**Purpose:** Initialize the deep learning environment.\n",
    "**How it works:**\n",
    "* **Device Detection:** Automatically detects if your Mac has an M-series chip (`mps`) or if you are on a standard CPU (`cpu`). This ensures the model runs as fast as possible.\n",
    "* **Path Configuration:** Dynamically finds your `data/processed` folder so we can load the manifest we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce26410-798d-4ec0-916d-db0aa165551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Check if GPU is available (mps for Mac, cuda for Nvidia, cpu otherwise)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using Device: {device}\")\n",
    "\n",
    "# Paths\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.dirname(NOTEBOOK_DIR)\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'processed', 'training_manifest.csv')\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, 'src')\n",
    "\n",
    "# Add src to path\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, SRC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489cf52-6458-48b7-b321-d294603827ec",
   "metadata": {},
   "source": [
    "### 2. The DNA Tokenizer\n",
    "**Purpose:** Translate biological language (DNA) into machine language (Numbers).\n",
    "**How it works:**\n",
    "* **Vocabulary:** We define a simple map: `A=2`, `T=3`, `C=4`, `G=5`.\n",
    "* **Encoding:** The `encode` function takes a string like `\"ATCG\"`, looks up the numbers, and returns a Tensor `[2, 3, 4, 5]`.\n",
    "* **Padding:** Since neural networks need fixed-size inputs (e.g., exactly 2000 bp), we fill any empty space with `0` (PAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1142068-d9b9-4fd0-984e-9cdac4f2e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ATCGGGCTA\n",
      "Encoded:  tensor([2, 3, 4, 5, 5, 5, 4, 3, 2, 0])\n",
      "Decoded:  ATCGGGCTA\n"
     ]
    }
   ],
   "source": [
    "class DNATokenizer:\n",
    "    def __init__(self):\n",
    "        # K-mer tokenization (using k=1 for simplicity, can upgrade to k=3 later)\n",
    "        self.vocab = {'PAD': 0, 'UNK': 1, 'A': 2, 'T': 3, 'C': 4, 'G': 5, 'N': 6}\n",
    "        self.pad_token_id = 0\n",
    "    \n",
    "    def encode(self, seq, max_length=1000):\n",
    "        \"\"\"Converts string sequence to list of integers.\"\"\"\n",
    "        # Truncate if too long\n",
    "        seq = seq[:max_length]\n",
    "        \n",
    "        # Map chars to ints\n",
    "        ids = [self.vocab.get(char, self.vocab['UNK']) for char in seq.upper()]\n",
    "        \n",
    "        # Padding\n",
    "        if len(ids) < max_length:\n",
    "            ids += [self.pad_token_id] * (max_length - len(ids))\n",
    "            \n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Converts integers back to string (for checking).\"\"\"\n",
    "        rev_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        chars = [rev_vocab.get(i.item(), '?') for i in ids if i.item() != 0]\n",
    "        return \"\".join(chars)\n",
    "\n",
    "# Test it\n",
    "tokenizer = DNATokenizer()\n",
    "test_seq = \"ATCGGGCTA\"\n",
    "encoded = tokenizer.encode(test_seq, max_length=10)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Original: {test_seq}\")\n",
    "print(f\"Encoded:  {encoded}\")\n",
    "print(f\"Decoded:  {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac863a-d34b-40c0-a1ad-ca5043e94429",
   "metadata": {},
   "source": [
    "### 3. The Genomic Dataset (The Pipeline)\n",
    "**Purpose:** Stream data from disk to the model during training.\n",
    "**How it works:**\n",
    "* **Lazy Loading:** Instead of loading 100,000 sequences into RAM at once, we load the *Genome Index* once.\n",
    "* **On-the-Fly Extraction:** When the model asks for \"Item #42\", this class looks up the row in the CSV, jumps to the exact coordinate in the Genome file, extracts the sequence, tokenizes it, and hands it to the model. This is extremely memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f96a5c-0323-4ab5-a584-acd20d839258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Genome for fast lookup...\n",
      "Batch Shape: torch.Size([4, 2000])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Bio import SeqIO\n",
    "import gzip\n",
    "\n",
    "class GenomicDataset(Dataset):\n",
    "    def __init__(self, manifest_path, genome_path, tokenizer, max_length=2000):\n",
    "        self.manifest = pd.read_csv(manifest_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load Genome into Memory (Fast lookup)\n",
    "        print(\"Loading Genome for fast lookup...\")\n",
    "        with gzip.open(genome_path, \"rt\") as handle:\n",
    "            self.genome = SeqIO.to_dict(SeqIO.parse(handle, \"fasta\"))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.manifest.iloc[idx]\n",
    "        \n",
    "        # Extract Sequence\n",
    "        chrom = row['chrom']\n",
    "        # Handle chromosome naming mismatches\n",
    "        if chrom not in self.genome:\n",
    "            if f\"chr{chrom}\" in self.genome: chrom = f\"chr{chrom}\"\n",
    "            elif chrom.replace('chr', '') in self.genome: chrom = chrom.replace('chr', '')\n",
    "        \n",
    "        # Get raw string\n",
    "        raw_seq = str(self.genome[chrom].seq[row['start']-1 : row['end']])\n",
    "        \n",
    "        # Tokenize\n",
    "        input_ids = self.tokenizer.encode(raw_seq, self.max_length)\n",
    "        \n",
    "        # Target (Label): For now, we predict GC content as a dummy task\n",
    "        # Later, this will be expression levels.\n",
    "        label = torch.tensor(row['gc_content'], dtype=torch.float32)\n",
    "        \n",
    "        return input_ids, label\n",
    "\n",
    "# Initialize Dataset\n",
    "GENOME_PATH = os.path.join(PROJECT_ROOT, 'data', 'raw', 'dm6.fa.gz')\n",
    "dataset = GenomicDataset(DATA_PATH, GENOME_PATH, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Test a batch\n",
    "sample_inputs, sample_labels = next(iter(dataloader))\n",
    "print(f\"Batch Shape: {sample_inputs.shape}\") # Should be [4, 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd065f6-19d3-44b4-b10f-e5f7bad3638f",
   "metadata": {},
   "source": [
    "### 4. The Model: DNA-BERT\n",
    "**Purpose:** The \"Brain\" of our project. A Transformer-based architecture designed to learn patterns in DNA sequences.\n",
    "**Structure:**\n",
    "1.  **Embedding Layer:** Converts integer tokens (e.g., `5` for G) into rich vectors (lists of 128 numbers) representing that base's properties.\n",
    "2.  **Positional Encoding:** Adds information about *order*. In DNA, \"ATG\" (Start) is very different from \"TGA\" (Stop), so the model needs to know which letter comes first.\n",
    "3.  **Transformer Encoder:** The heavy lifter. It uses \"Self-Attention\" to look at the whole sequence at once and understand how base pairs interact, even if they are far apart.\n",
    "4.  **Regressor Head:** A final simple layer that boils the complex understanding down to a single number (Prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b851ef-b926-47c3-b3e5-55848ab49768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNABert(\n",
      "  (embedding): Embedding(7, 128)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (regressor): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DNABert(nn.Module):\n",
    "    def __init__(self, vocab_size=7, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embeddings: Convert IDs (0,1,2..) to Vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding (Simple learned version)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 4. Regression Head (Predicts a single number: GC/Expression)\n",
    "        self.regressor = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Add embedding + pos encoding\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # Pass through Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Average Pool (Combine all positions into one vector)\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Final Prediction\n",
    "        x = self.regressor(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Instantiate Model\n",
    "model = DNABert().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d695d-76d0-4685-9d78-270fc5c7f616",
   "metadata": {},
   "source": [
    "### 5. Verification: The \"Dry Run\"\n",
    "**Purpose:** Safety check.\n",
    "**How it works:**\n",
    "* We grab a single batch of real data from our new DataLoader.\n",
    "* We push it through the untrained model.\n",
    "* **Goal:** We want to see if it *crashes*. If we see \"Forward Pass Successful\" and a predicted number (even if it's random/wrong), it means the \"plumbing\" is connected correctly. We are ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61df9f6b-1546-4419-9cb6-ce54f44d852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Starting Dry Run...\n",
      "\n",
      "âœ… Forward Pass Successful!\n",
      "Input Shape:  torch.Size([4, 2000])\n",
      "Output Shape: torch.Size([4])\n",
      "Example Prediction: 0.3524\n",
      "Actual Target:      0.4741\n",
      "\n",
      "ðŸš€ Ready for Training Loop in Notebook 03!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ§ª Starting Dry Run...\")\n",
    "\n",
    "# 1. Get Batch\n",
    "inputs, targets = next(iter(dataloader))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# 2. Pass through Model\n",
    "outputs = model(inputs)\n",
    "\n",
    "# 3. Check Outputs\n",
    "print(\"\\nâœ… Forward Pass Successful!\")\n",
    "print(f\"Input Shape:  {inputs.shape}\")\n",
    "print(f\"Output Shape: {outputs.shape}\")\n",
    "print(f\"Example Prediction: {outputs[0].item():.4f}\")\n",
    "print(f\"Actual Target:      {targets[0].item():.4f}\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for Training Loop in Notebook 03!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14f06f-3ab2-4276-83af-713106b0ff6b",
   "metadata": {},
   "source": [
    "# âœ… Architecture Verified\n",
    "\n",
    "**Status:** Ready for Training.\n",
    "\n",
    "We have successfully:\n",
    "1.  **Built the Engine:** The DNA-BERT model is defined and compiles without errors.\n",
    "2.  **Connected the Fuel:** The Data Loader is successfully pulling DNA sequences from our raw genome file and tokenizing them.\n",
    "3.  **Test Start:** The \"Dry Run\" confirmed that data flows through the model and produces a prediction output.\n",
    "\n",
    "### ðŸš€ Next Step: Notebook 03\n",
    "Now that the car is built and the fuel line is connected, it's time to teach it how to drive.\n",
    "In **`03_Model_Training.ipynb`**, we will:\n",
    "* Define the **Loss Function** (how the model measures its mistakes).\n",
    "* Set up the **Optimizer** (how the model learns).\n",
    "* Run the **Training Loop** to actually teach the AI to predict GC content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Genomic AI)",
   "language": "python",
   "name": "genomic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
